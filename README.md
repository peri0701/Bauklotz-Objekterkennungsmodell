![Banner](https://github.com/peri0701/Bauklotz-Objekterkennungsmodell/blob/main/Bilder%20&%20Videos%20f%C3%BCr%20die%20GitHub%20Seite/Banner_GitHub_Feb2023.png?raw=true)
[![√ñffnen in Colab](https://img.shields.io/badge/Open%20in%20Colab-grey?style=flat-square&logo=google-colab&logoColor=F9AB00&labelColor=grey&color=F9AB00)](https://colab.research.google.com/drive/1halSc73m1BsovgJhM2FQkquQVUpD2_7B?usp=sharing)


Willkommen auf der GitHub Seite zu meiner **Bachelorarbeit**:  
**Entwicklung einer KI-basierten Objekterkennungsapplikation f√ºr einen Industrieroboter**.  

---

## üéØ Projekt√ºbersicht

In meiner Bachelorarbeit habe ich ein YOLOv8s-Modell trainiert, das speziell f√ºr die Objekterkennung von Baukl√∂tzen entwickelt wurde und auf dem Raspberry Pi 5 mit dem AI Kit sowie der Camera Module 3 ausf√ºhrbar ist.

Diese GitHub-Seite bietet:
- **Zugriff auf wichtige Dokumente, Codes und Ergebnisse**, um die Arbeit f√ºr Leser und Nutzer nachvollziehbar zu machen.
- **Detaillierte Repositories**, die Aspekte wie Modelltraining und Datensatzaufbereitung beleuchten.
---

### üìÇ Schritte der Projektumsetzung

1. Erstellen eines benutzerdefinierten Datensatzes mit Roboflow
2. Trainieren des Modells und Validierung der Ergebnisse
3. Ausf√ºhren des Modells auf dem Raspberry Pi 5
4. Konvertieren des Modells in das ONNX-Format
5. Konvertieren des ONNX-Modells in das HEF-Format
6. Ausf√ºhren des konvertierten Modells auf dem Raspberry Pi 5 & AI Kit
   
Diese Schritte umfassen den gesamten Entwicklungsprozess ‚Äì von der Datensammlung √ºber das Modelltraining bis hin zur finalen Anwendung auf der Hardware. Im Folgenden wird auf jede Projektphase detailliert eingegangen, um die Umsetzung und die verwendeten Methoden n√§her zu erl√§utern.

---

## üì∏ Erstellen eines benutzerdefinierten Datensatzes mit Roboflow
[Roboflow](https://roboflow.com/) bietet eine √ºbersichtliche Plattform zur Erstellung, Annotation (Abgrenzungen von Objekten in Bildern) und Verwaltung von Datens√§tzen. Mit √ºber 110.000 √∂ffentlich zug√§nglichen Datens√§tzen in [Roboflow Universe](https://universe.roboflow.com/) ist es ein vielseitiges Werkzeug f√ºr verschiedene Anwendungsbereiche. 

Im Folgenden wird gezeigt, wie ein benutzerdefinierter Datensatz erstellt werden kann.


### Schritt 1: Projekt erstellen:
Zum Erstellen eines neuen Projekts ist die [Registrierung bei Roboflow](https://app.roboflow.com/login) erforderlich. Nach der Registrierung kann ein Workspace benannt und ein neues Projekt im Dashboard angelegt werden. Die kostenlose Variante des Public Plans ist hierf√ºr ausreichend:

<img src="https://github.com/peri0701/Bauklotz-Objekterkennungsmodell/blob/main/Bilder%20&%20Videos%20f%C3%BCr%20die%20GitHub%20Seite/Video3%20(10).gif?raw=true" alt="Demo" width="600">

### Schritt 2: Bilder hochladen:
Bilder k√∂nnen in das neu erstellte Projekt hochgeladen werden.
**Hinweis**: Falls ein Datensatz bereits annotierte Dateien enth√§lt, k√∂nnen diese in Roboflow hochgeladen werden, wo sie automatisch erkannt und den entsprechenden Bildern zugeordnet werden:

<img src="https://github.com/peri0701/Bauklotz-Objekterkennungsmodell/blob/main/Bilder%20&%20Videos%20f%C3%BCr%20die%20GitHub%20Seite/Video3%20(6).gif?raw=true" alt="Demo" width="600">


### Schritt 3: Labeln & Annotation:

- **Manuelles Annotieren:**
Diese Methode bietet maximale Kontrolle √ºber die Pr√§zision der Annotationen. Objekte k√∂nnen im Bild markiert und mit passenden Labels versehen werden. Allerdings ist dieser Prozess zeitaufwendig:

<img src="https://github.com/peri0701/Bauklotz-Objekterkennungsmodell/blob/main/Bilder%20&%20Videos%20f%C3%BCr%20die%20GitHub%20Seite/Video3%20(3).gif?raw=true" alt="Demo" width="600">

- **Auto-Labeling:**
Die Auto-Labeling-Funktion automatisiert den Annotierungsprozess, indem sie nach einem kurzen begleiteten Training, bei dem das Objekt der KI beschrieben wird, die Objekte im Bild erkennt und den restlichen Datensatz automatisch labelt. Diese Funktion ist besonders hilfreich, um Zeit zu sparen. Automatisch generierte Labels k√∂nnen dann √ºberpr√ºft und bei Bedarf angepasst werden:

<img src="https://github.com/peri0701/Bauklotz-Objekterkennungsmodell/blob/main/Bilder%20&%20Videos%20f%C3%BCr%20die%20GitHub%20Seite/Video3%20(9).gif?raw=true" alt="Demo" width="600">

**Hinweis**: Nach dem Labeln und Annotieren wird die Aufteilung des Datensatzes in die drei Kategorien train, test und val abgefragt. Diese Trennung sollte best√§tigt werden, da sie f√ºr das sp√§tere Modelltraining von Bedeutung ist.

### Schritt 4: Neue Datensatzversion erstellen
Nach Abschluss der Annotationen kann eine neue Version des Datensatzes generiert werden. YOLO-Modelle werden in einem quadratischen Bildformat trainiert, weshalb der Datensatz in dieser Arbeit auf die Bildgr√∂√üe 640x640 skaliert wurde. Zus√§tzlich k√∂nnen Augmentationsmethoden wie Drehen, Skalieren oder das Anpassen von Helligkeit, Kontrast und S√§ttigung genutzt werden, um die Datenvielfalt und -robustheit zu erh√∂hen.

<img src="https://github.com/peri0701/Bauklotz-Objekterkennungsmodell/blob/main/Bilder%20&%20Videos%20f%C3%BCr%20die%20GitHub%20Seite/Video3%20(5).gif?raw=true" alt="Demo" width="600">

### Schritt 5: Datensatz exportieren
Der Datensatz steht nach der Generierung in verschiedenen Formaten, darunter im YOLOv8-Format, zum Export bereit. Der Datensatz sollte lokal auf dem Rechner gespeichert werden, da er f√ºr die sp√§tere Konvertierung ben√∂tigt wird, der Export kann auch √ºber die API direkt in eine Trainingsumgebung wie Google Colab integriert werden

Im Beispiel wird gezeigt, wie mein finaler Datensatz, der f√ºr diese Arbeit erstellt wurde, in [Roboflow Universe](https://universe.roboflow.com/) gefunden und heruntergeladen werden kann.
Der Datensatz ist ebenfalls √ºber folgenden [Direktlink](https://www.google.com/url?q=https%3A%2F%2Funiverse.roboflow.com%2Fbauklotz%2Fbauklotz-c8zsq%2Fdataset%2F1) einsehbar:

<img src="https://github.com/peri0701/Bauklotz-Objekterkennungsmodell/blob/main/Bilder%20&%20Videos%20f%C3%BCr%20die%20GitHub%20Seite/Video3%20(11).gif?raw=true" alt="Demo" width="600">

‚úî **Hinweis**: Nach dem Export wird empfohlen, den Datensatz erneut zu importieren, um automatisch generierte Annotationen zu √ºberpr√ºfen und bei Bedarf anzupassen.

---

## 2. **Trainieren des Modells und Validierung der Ergebnisse**

Das Modelltraining wird in [Google Colab](https://colab.research.google.com/drive/1halSc73m1BsovgJhM2FQkquQVUpD2_7B?usp=sharing) durchgef√ºhrt, um die Datei **best.pt** zu generieren. Dabei handelt es sich um eine PyTorch-Datei, die die optimierte Version des trainierten Modells enth√§lt und als Basis f√ºr die Modellausf√ºhrung auf dem Raspberry Pi 5 verwendet wird. √úber [diesen Link](https://colab.research.google.com/drive/1halSc73m1BsovgJhM2FQkquQVUpD2_7B?usp=sharing) kann das Modelltraining in Google Colab ausgef√ºhrt werden.

---

## 3. **Modellausf√ºhrung auf dem Raspberry PI 5**
#### √úbertragen der Modell-Datei
Nach Erhalt der Datei **best.pt** kann diese √ºber Google Drive auf den Raspberry Pi 5 heruntergeladen werden.

#### Einrichten der virtuellen Umgebung
F√ºr die Modellausf√ºhrung ist zun√§chst eine virtuelle Umgebung erforderlich. Diese isoliert das Projekt, sodass es unabh√§ngig vom restlichen Betriebssystem und anderen installierten Paketen ausgef√ºhrt werden kann, ohne die Stabilit√§t des Systems zu gef√§hrden.

Mit folgendem Befehl wird eine neue virtuelle Umgebung namens **env** erstellt. Die Option **--system-site-packages** stellt sicher, dass die Umgebung Zugriff auf global installierte Python-Pakete hat:

```bash
python3 -m venv --system-site-packages env
```
```bash
source env/bin/activate
```

#### Systemaktualisierung und Paketinstallation
Das System wird aktualisiert, um sicherzustellen, dass die neuesten Paketlisten verf√ºgbar sind und zuk√ºnftige Installationen reibungslos verlaufen. Zus√§tzlich wird der Python-Paketmanager installiert und auf die neueste Version aktualisiert, um die Kompatibilit√§t mit modernen Bibliotheken zu gew√§hrleisten:

```bash
sudo apt update
sudo apt install python3-pip -y
pip install -U pip
```

#### Installation der YOLOv8-Bibliothek
Die YOLOv8-Bibliothek von Ultralytics wird zusammen mit allen f√ºr den Export erforderlichen Paketen installiert. Dies erm√∂glicht die Durchf√ºhrung der Objekterkennung mit YOLO auf dem Raspberry Pi:

```bash
pip install ultralytics[export]
```
#### Systemneustart

Ein Neustart des Systems stellt sicher, dass alle vorgenommenen √Ñnderungen und Installationen √ºbernommen wurden:

```bash
sudo reboot
```
### Nutzung von Thonny
Thonny ist eine benutzerfreundliche Entwicklungsumgebung f√ºr Python, die sich ideal f√ºr Projekte wie die Objekterkennung mit YOLO auf dem Raspberry Pi eignet. Sie erm√∂glicht das Arbeiten in virtuellen Umgebungen (venv), wodurch Skripte sicher getestet und ausgef√ºhrt werden k√∂nnen, ohne das Hauptsystem des Raspberry Pi zu beeintr√§chtigen. Dies erleichtert die Entwicklung direkt auf der Plattform. 

Innerhalb von Thonny kann die entsprechende virtuelle Umgebung ausw√§hlt erden.
Dadurch bleiben alle Skripte und Pakete des Projekts in der isolierten Umgebung und beeinflussen nicht das globale System.

Im folgenden Video wird gezeigt, wie die Verkn√ºpfung von Thonny mit der venv eingerichtet wird:

<img src="https://github.com/peri0701/Bauklotz-Objekterkennungsmodell/blob/main/Bilder%20%26%20Videos%20f%C3%BCr%20die%20GitHub%20Seite/Video3%20(12).gif?raw=true" alt="Demo" width="600">

#### Speicherort der virtuellen Umgebung und Ausf√ºhrung des Modells

Der Ordner der virtuellen Umgebung (venv) wird standardm√§√üig im aktuellen Arbeitsverzeichnis erstellt, in dem der entsprechende Befehl ausgef√ºhrt wurde. Auf dem Raspberry Pi befindet sich dieses Verzeichnis in der Regel im Home-Verzeichnis des Benutzers, beispielsweise unter **/home/robolab/**. Wird die virtuelle Umgebung mit dem Namen **env** erstellt, liegt diese anschlie√üend im Home-Verzeichnis unter **/home/robolab/env**, sofern kein alternatives Verzeichnis angegeben wurde.

Das folgende Skript sollte in die Benutzeroberfl√§che von Thonny eingef√ºgt werden. Es dient der Ausf√ºhrung des Modells (**best.pt**), das zuvor im Ordner der virtuellen Umgebung gespeichert wurde:


```python
import cv2
from picamera2 import Picamera2
from ultralytics import YOLO

# Kamera mit Picamera2 einrichten
picam2 = Picamera2()
picam2.preview_configuration.main.size = (800, 800)
picam2.preview_configuration.main.format = "RGB888"
picam2.preview_configuration.align()
picam2.configure("preview")
picam2.start()

# YOLOv8s-Modell laden
model = YOLO("best.pt")  # Enable YOLO's default logging

# Klassennamen definieren
class_names = {0: "Bauklotz"}  # Verkn√ºpft Klassenindizes mit Klassennamen

while True:
    # Einzelbild von der Kamera erfassen
    frame = picam2.capture_array()
    
    # YOLO-Modell auf das erfasste Bild anwenden und Ergebnisse speichern
    results = model(frame)
    detections = results[0].boxes.xyxy  # Extrahiere Koordinaten der Begrenzungsrahmen
    classes = results[0].boxes.cls     # Extrahiere Klassenindizes
    confidences = results[0].boxes.conf  # Extrahiere Konfidenzwerte

    # Zus√§tzliche Erkennungsdetails mit Koordinaten ausgeben
    print("\nKoordinaten:")
    for i, box in enumerate(detections):
        x1, y1, x2, y2 = map(int, box)  # Koordinaten als absolute Pixelwerte
        class_index = int(classes[i])
        class_name = class_names.get(class_index, "Unknown") # Klassennamen anhand des Index abrufen
        confidence = confidences[i] # Konfidenzwert f√ºr die aktuelle Erkennung

        # Erkennungsdetails ausgeben
        print(f"Object {i+1}: {class_name}, Confidence: {confidence:.2f}, Coordinates: ({x1}, {y1}, {x2}, {y2})")

        # Bounding  Box und Labels auf das Bild zeichnen
        label = f"{class_name} {confidence:.2f}"  # Klasse und Konfidenz anzeigen
        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Bounding box (Begrenzungsrahmen)
        # Gr√∂√üe des Labels anpassen f√ºr bessere Lesbarkeit
        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2, cv2.LINE_AA)

    # Bild mit den Ergebnissen anzeigen
    cv2.imshow("Camera", frame)

    # Programm beenden, wenn "q" gedr√ºckt wird
    if cv2.waitKey(1) == ord("q"):
        break

# Alle Fenster schlie√üen
cv2.destroyAllWindows()
```
Hier ein Beipiel von der Ausgabe meines Modells:


![image](https://github.com/user-attachments/assets/0cdf7362-b92e-4f8e-a2b4-e03df282c4dd)



---

## 4. **Modellkonvertierung in ONNX**
Die Hailo SDK und der Hailo Data Compiler, die sp√§ter f√ºr die Konvertierung des ONNX-Modells in das Hailo Execution Format (HEF) verwendet werden, unterst√ºtzen nur ONNX-Modelle bis zu einer bestimmten Opset-Version, die Versionen die ich genutzt habe besitzen eine Obergrenze bei Opset 9. Die Opset-Version definiert die Funktionen und Operatoren, die innerhalb des Modells verwendet werden k√∂nnen. Opset 9 ist eine stabile und weit unterst√ºtzte Version, die mit den meisten √§lteren Frameworks und Tools kompatibel ist, insbesondere mit der HEF-Pipeline.

Die Konvertierung von ONNX kann direkt √ºber eine YOLO CLI von den zuvor heruntergeladenen Ultralytics Paketen durchgef√ºhrt werden kann:
```bash
cd env
yolo export model=best.pt imgsz=640 format=onnx opset=9
```
Nach dem Durchf√ºhren des Befehls, erh√§lt man folgende Ausgabe und die Information, das die ONNX Datei im selben Ordner, unserer venv, bei mir in "env" gespeichert wurde:

![Image](https://github.com/peri0701/Bauklotz-Objekterkennungsmodell/blob/main/Bilder%20%26%20Videos%20f%C3%BCr%20die%20GitHub%20Seite/onnx_conversion.png)

---
## 5. **Modellkonvertierung in HEF**

### 1.Linux-Umfeld herstellen (WSL & Ubuntu 22.04)
Um ein trainiertes Modell auf dem Raspberry Pi AI Kit mit der Hailo-Hardware auszuf√ºhren, ist eine Konvertierung des Modells in das Hailo Execution Format (HEF) notwendig. Da die Hailo-Software derzeit ausschlie√ülich auf x86-Linux-Systemen unterst√ºtzt wird, bietet die Nutzung von WSL (Windows Subsystem for Linux) eine einfache L√∂sung f√ºr Windows-Nutzer. 

Die Ubuntu 22.04-Version wird zus√§tzlich √ºber den Microsoft Store heruntergeladen, um eine separate Linux-Applikation bereitzustellen. Obwohl dieser Schritt im urspr√ºnglichen Tutorial nicht vorgesehen war, hat sich die Methode als erfolgreich erwiesen und bietet den Vorteil einer isolierten Arbeitsumgebung mit erh√∂hter Flexibilit√§t ‚Äì weshalb ich diese Methode ebenfalls empfehlen w√ºrde:

<img src="https://github.com/user-attachments/assets/89f280d0-903b-4719-9b9d-d328ea8e20bd"  width="400">

Mit WSL kann eine Ubuntu-Umgebung direkt auf Windows eingerichtet werden, wodurch der gesamte Prozess der Modellkonvertierung wie auf einem nativen Linux-System durchgef√ºhrt werden kann, dazu muss zun√§chst die Powershell ge√∂ffnet werden:

```powershell
wsl --list --online
```
```powershell
wsl --install -d Ubuntu-22.04
```
Nach diesem Schritt wird gebeten ein Benutzername und Passwort anzugeben. Das Passwort wird bei allen "sudo" Befehlen angefragt im Ubutu Umfeld

```powershell
sudo apt update
```
```powershell
sudo apt upgrade
```
Ab hier kann bei Bedarf in die Applikation gewechselt werden.

### 2. Hailo Data Compiler herunterladen 

Der Hailo Dataflow Compiler wird ben√∂tigt, um ONNX-Modelle in das f√ºr die Hailo-Hardware optimierte HEF-Format zu konvertieren. Er passt Modelle an die Hardwarearchitektur an und erm√∂glicht durch Optimierungen wie Quantisierung eine maximale Leistung bei minimalem Ressourcenverbrauch.

NUn werden erforderliche Python Pakete heruntergeladen, die wir f√ºr den Hailo Data Compiler ben√∂tigen.

```powershell
sudo apt install python3-pip
sudo apt install python3.10-venv
sudo apt-get install python3.10-dev python3.10-distutils python3-tk libfuse2 graphviz libgraphviz-dev
sudo pip install pygraphviz
sudo apt install wslu
```
Eine venv aufgestellt:

```powershell
python3 -m venv hailodfc
. hailodfc/bin/activate
```

Um die Kompalit√§t mit der python Umgebung u sehen - Sie sollte 3.10.12 sein.
```powershell
python3 --version
```

Um den Hailo Dataflower Compiler herunterzuladen muss erst eine Registrierung in die Hailo Developer Zone erfolgen, die [hier√ºber](https://hailo.ai/authorization/?redirect_to=https%3A%2F%2Fhailo.ai%2Fdeveloper-zone%2Fsoftware-downloads%2F) durchgef√ºhrt werden kann. Als n√§chstes muss die 3.28 Version mit der Python Version 3.10, die am 1.Juli 2024 erschienen ist heruntergeladen werden.

<img src="https://github.com/peri0701/Bauklotz-Objekterkennungsmodell/blob/main/Bilder%20&%20Videos%20f%C3%BCr%20die%20GitHub%20Seite/Video3%20(13).gif?raw=true" alt="Demo" width="600">

Nun muss die heruntergelade Datei in unser Ubuntu "home" Verzeichnis gebracht und abgelegt werden, damit sie in unserem Ubuntu Umfeld auch installiert werden kann, mit folgendem Befehl, √∂ffnet sich das Zielverzeichnis, die Datei muss bon den Downloads in diesen Bereich verschoben werden:

```powershell
wslview .
```

<img src="https://github.com/peri0701/Bauklotz-Objekterkennungsmodell/blob/main/Bilder%20&%20Videos%20f%C3%BCr%20die%20GitHub%20Seite/dataflower.jpeg?raw=true"  width="400">

Ist dies erledigt, kann mit dem Befehl, wird die Installation gestartet: 
```powershell
pip3 install hailo_dataflow_compiler-3.28.0-py3-none-linux_x86_64.whl
```

Mt den Befehlen, kann verifiziert werden, ob der Download erfolgreich war, indem man "help" anzeigen l√§sst:

```powershell
hailo -h
```
![image](https://github.com/user-attachments/assets/990df005-46d5-4626-979c-c5db122b57f1)


### 3. Hailo Model Zoo herunterladen

In einem n√§chsten Schritt wird Hailo Modek Zoo heruntergeladen, dazu nutzen wir Hailo AIs Github repository:

```powershell
git clone https://github.com/hailo-ai/hailo_model_zoo.git
```

Wechseln das Verzeichnis und installieren alle darin enthaltenen Pakete die zum Setup dazu geh√∂ren:

```powershell
cd hailo_model_zoo; pip install -e .
```

## 5. Vorbereitung auf die HEF Konverterierung 

Nun muss wie vorher die heruntergeladene Hailo Datafllow Compiler auch unsere zuvor enerierte best.onnx Datei in das Ubuntu Verzeichnis gezogen werden - Vom Raspberry Pi kann diese √ºber Google Drive auf dem Rechner heruntergeladen werden.

<img src="https://github.com/peri0701/Bauklotz-Objekterkennungsmodell/blob/main/Bilder%20&%20Videos%20f%C3%BCr%20die%20GitHub%20Seite/best.jpeg?raw=true"  width="400">

Beim Herunterladen des Datensatzes wurdn die Bilder und Annotationen in drei Ordner Strukturen aufgeteilt, der train Ordner, muss ebenfalls in das Ubuntu Umfeld gepackt werden.


<img src="https://github.com/peri0701/Bauklotz-Objekterkennungsmodell/blob/main/Bilder%20&%20Videos%20f%C3%BCr%20die%20GitHub%20Seite/trsin_folder.jpeg?raw=true" width="400">

Viele machen den Fehler den Befehl zur Performance Steigerung jetzt schon auszugeben, da man davon ausgehen kann, dass man alle Komponenten besitzt:


```powershell
hailomz compile yolov8s --ckpt=best.onnx --hw-arch hailo8l --calib-path train/images --classes 1 --performance
```
Im Hintergrund m√ºssen im hailo model zoo, folgende Skripte angepasst werden, die unter folgendem Verzeichnis zu finden sind im Ubuntu Verzeichnis:

# 
![image](https://github.com/user-attachments/assets/4172ebb3-c527-4208-8819-20472d413967)

Gegebenfalls anpassen, mein Model ist auf ein 640x640 Format trainiert worden  # hier die Klassenanzahl angeben, hier 1

```json
{
	"nms_scores_th": 0.2,
	"nms_iou_th": 0.7,
	"image_dims": [
		640,
		640 
	],
	"max_proposals_per_class": 100,
	"classes": 1,
	"regression_length": 16,
	"background_removal": false,
	"background_removal_index": 0,
	"bbox_decoders": [
		{
			"name": "bbox_decoder41",
			"stride": 8,
			"reg_layer": "conv41",
			"cls_layer": "conv42"
		},
		{
			"name": "bbox_decoder52",
			"stride": 16,
			"reg_layer": "conv52",
			"cls_layer": "conv53"
		},
		{
			"name": "bbox_decoder62",
			"stride": 32,
			"reg_layer": "conv62",
			"cls_layer": "conv63"
		}
	]
}
```

#
![image](https://github.com/user-attachments/assets/2905ce2d-8423-4528-ace4-11579f15bf11)


```plaintext
quantization_param([conv42, conv53, conv63], force_range_out=[0.0, 1.0])
normalization1 = normalization([0.0, 0.0, 0.0], [255.0, 255.0, 255.0])
change_output_activation(conv42, sigmoid)
change_output_activation(conv53, sigmoid)
change_output_activation(conv63, sigmoid)
performance_param(compiler_optimization_level=max)
nms_postprocess("../../postprocess_config/yolov8s_nms_config.json", meta_arch=yolov8, engine=cpu)
```


#

<img src="https://github.com/peri0701/Bauklotz-Objekterkennungsmodell/blob/main/Bilder%20&%20Videos%20f%C3%BCr%20die%20GitHub%20Seite/network.jpeg?raw=true" width="400">

```yaml
base:
- base/yolov8.yaml
postprocessing:
  device_pre_post_layers:
    nms: true
  hpp: true
network:
  network_name: yolov8s
paths:
  network_path:
  - best.onnx #Hier muss der genaue Pfad zur ONNX Datei angegeben werden 
  alls_script: hailo_model_zoo/hailo_model_zoo/cfg/alls/yolov8s.alls #Hier muss der genaue Pfad zur yolov8s.alls Datei angegeben werden 
  url: https://hailo-model-zoo.s3.eu-west-2.amazonaws.com/ObjectDetection/Detection-COCO/yolo/yolov8s/2023-02-02/yolov8s.zip
parser:
  nodes:
  - null
  - - /model.22/cv2.0/cv2.0.2/Conv
    - /model.22/cv3.0/cv3.0.2/Conv
    - /model.22/cv2.1/cv2.1.2/Conv
    - /model.22/cv3.1/cv3.1.2/Conv
    - /model.22/cv2.2/cv2.2.2/Conv
    - /model.22/cv3.2/cv3.2.2/Conv
info:
  task: object detection
  input_shape: 640x640x3
  output_shape: 1x5x100 #Erste Zahl steht f√ºr die Klasse (1=Bauklotz)
  operations: 28.4G #Entspricht GFLOPs der "Model Summary(fused)"   Angabe meines Modeltrainings
  parameters: 11.125M #Entspricht den parametern der "Model Summary(fused)" des Modelltrainings 
  framework: pytorch
  training_data: coco train2017
  validation_data: coco val2017
  eval_metric: mAP
  full_precision_result: 44.75
  source: https://github.com/ultralytics/ultralytics
  license_url: https://github.com/ultralytics/ultralytics/blob/main/LICENSE
  license_name: GPL-3.0
```

Ist die Optimierung abgeschlossen, erh√§lt die Information, dass die Hef datei gespeichert wurde.
![image](https://github.com/user-attachments/assets/7183d63d-2382-4a96-a527-8c46464e1d64)

---
## 6. **Modellausf√ºhrung auf dem Raspberry PI 5 & Ai KIT**

Daf√ºr muss das entsprechende Umfeld erst geschaffen werden, daf√ºr wird das Hailo rpi5 Examples repository auf dem Raspberry Pi heruntergeladen.

```bash
git clone https://github.com/hailo-ai/hailo-rpi5-examples.git
```
```bash
cd hailo-rpi5-examples
```
```bash
./install.sh
```
Falls der ./install.sh nicht auf anhieb funktionieren sollte, so gibt es eine Zweite M√∂glichkeit die ben√∂tigten Packete herunterzuladen, eine venv wird manuell durch folgenden Link erstellt, hier: 

```bash
source setup_env.sh
```
```bash
pip install -r requirements.txt
```
```bash
./download_resources.sh
```
```bash
./compile_postprocess.sh
```

Wenn die venv geschlossen wird und man eine neue Terminal session benutzen m√∂chte mit den Paketen, so sollte immer source setup_env.sh verwendet werden. 

**Hinweis:** Man muss isch im hailo-rpi5-examples Verzeichnis befinden!
Wie folgt ist nach einer neuen Terminal Session vorzugehen:

```bash
cd hailo-rpi5-examples
```

```bash
source setup_env.sh
```

Es gibt im Hailo-rpi5-examples GitHub verschiede Beispiele, wie der Hailo AI Kit mit dem Raspberry Pi5 genutzt werden kann, au√üer der Object detection Funktion, [hier](https://github.com/hailo-ai/hailo-rpi5-examples/tree/main) ein Link zur Seite.


Um unsere HEF Datei nutzen zu k√∂nnen, m√ºssen folgende Dokumente in den folgenden Verzechnissen hinterlegt werden: 



<img src="https://github.com/peri0701/Bauklotz-Objekterkennungsmodell/blob/main/Bilder%20&%20Videos%20f%C3%BCr%20die%20GitHub%20Seite/pipeline.jpeg?raw=true" width="400">

<img src="https://github.com/peri0701/Bauklotz-Objekterkennungsmodell/blob/main/Bilder%20&%20Videos%20f%C3%BCr%20die%20GitHub%20Seite/ressources.jpeg?raw=true" width="400">

Folgender Befehl wird dann zum Ausf√ºhren ausgef√ºhrt, input hier ist unsere Raspberry PI 3 Camera Module Kamera:

```bash
python basic_pipelines/bauklotz-detection.py --labels-json resources/bauklotz-labels.json --hef resources/yolov8s-hailo8l-barcode.hef --input -rpi
```

Hier die Ausgabe:

![image](https://github.com/user-attachments/assets/916cb36c-8beb-4041-ba46-8535442ebe21)


---

## üìñ Zus√§tzliche Dokumentation

Detaillierte Anleitungen und Hintergr√ºnde zu den einzelnen Schritten:
- [Einleitung und Motivation](https://your-github-pages-link.com/introduction)
- [Theoretische Grundlagen](https://your-github-pages-link.com/fundamentals)
- [Tutorials f√ºr jede Phase](https://your-github-pages-link.com/tutorials)
- [Herausforderungen und L√∂sungsans√§tze](https://your-github-pages-link.com/challenges)

---

## üåü Warum wurde eine GitHub Seite erstellt?

Dieses Projekt zeigt, wie leistungsstarke, kosteneffiziente Hardware wie der Raspberry Pi in Kombination mit modernen KI-Modellen genutzt werden kann, um industrielle Anwendungen zu verbessern. Es soll anderen Entwicklern und Forschern als Inspiration und Anleitung dienen.

---

## üìé Weitere Links und Ressourcen

- [YOLO Dokumentation](https://github.com/ultralytics/yolov5)
- [Roboflow Plattform](https://roboflow.com/)
- [Hailo Developer Resources](https://developer.hailo.ai/)


